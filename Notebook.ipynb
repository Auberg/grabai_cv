{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contain the steps of how to solve for the Computer Vision problem of Grab-AI competition at https://www.aiforsea.com/computer-vision. The goal is to create an AI that is capable of automatically recognize the model and make of a car given the image. This section explains the approach taken to solve the problem, followed by each section of the steps filled with the codes and small snipets of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for this problem the approach used will be deep learning based model, which will be explained in detailed further below. Because of the usage of the model approach the feature engineering steps will be removed as the model will automatically learn it. The deep learning model is build using Keras framework with Tensorflow as it's backend. Several steps that are taken in order to fulfil the goal are as follow:\n",
    "\n",
    "1. Data analysis  \n",
    "First before doing anything we will do checking on the training and testing data. The distribution of the data will be analyzed to make sure than imbalance between the classes won't cause any problem with the model. Result of this step shows that for both the training and test data they have __very good distribution__ for each classes.  \n",
    "Also in this section validation data will be generated. However with remark to the number of training and testing data (almost the same amount), the validation data will be generated from the testing data. After that to get sense of the training data, an image grid by the size of 28\\*28 containing representation of the image are created using t-sne. \n",
    "\n",
    "\n",
    "2. Training Preparation  \n",
    "Preparing the callbacks and data generator for the model.\n",
    "\n",
    "\n",
    "3. Model Benchmark  \n",
    "Creating base model benchmark that is not so complex and fast to train. InceptionV3 is used as the model benchmark. The model are able to get __63.625%__ accuracy with testing data.\n",
    "\n",
    "\n",
    "4. Further Model  \n",
    "Two more complex model will be created for final comparison, both of which are based on SeResNet50 [(paper)](https://arxiv.org/pdf/1709.01507.pdf). The first model is trained using _learning rate reducer and scheduler_, while the second model is trained with [*Stochastic Gradient Descent with Restarts*](https://arxiv.org/abs/1608.03983) ([link to code](https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452)) and [*Snapshot Ensemble* (which require SGDR)](https://arxiv.org/abs/1704.00109) ([link to code](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/callbacks/snapshot.py)). Result of the SeResNet50 first model shows __71.316%__ accuracy with testing data, while the second model perform at __51.938%__ accuracy. It seems that the implementation of the Snapshot Ensemble is not fully correct, causing the training not to run as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory of the data is as follow: (FILES)  \n",
    "__Notebook.ipynb__ : This file  \n",
    "__hollance_model.py__ : python file used in creating SeResNet50  \n",
    "__tsne_data_prep.py__ : python file used for copying data file for tsne  \n",
    "__tsne_grid.py__ : python file used for creating image grid based on tsne  \n",
    "__SENet2_params.npy__ : numpy file containing the seresnet50 weight  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory of the data is as follow: (FOLDER)   \n",
    "__data__  : Data folder. For train, test, and validation folder the image data is saved on different folder, each folder representing one class.  \n",
    " ---> car_ims : raw data of images  \n",
    " ---> car_devkit : devkit of car   \n",
    " ---> cars_train : train data folder  \n",
    " ---> cars_test : test data folder  \n",
    " ---> cars_vald : validation data folder   \n",
    " \n",
    "__parser__ : Folder containing weight of SeResNet50 (in caffe) and it's code to parse them to Keras.  \n",
    " ---> seresnet_weight : SeResNet50 weight  \n",
    " ---> weight_parser_hollance_original : code to convert the weight and original code of hollance_model.py  \n",
    "  \n",
    "__weight__ : Folder containing inceptionv3 and seresnet50 model weights  \n",
    " ---> inceptionv3 : self explanatory   \n",
    " ---> seresnet50 : self explanatory    \n",
    "\n",
    "__weights__ : Folder containing the ensemble model weights\n",
    "\n",
    "__tsne_grid__ : result of tsne_grid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "K.set_session(sess)\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    if not os.path.exists(path): \n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "label_path = os.path.join(data_path, 'car_devkit/devkit')\n",
    "train_path = os.path.join(data_path,'cars_train')\n",
    "vald_path = os.path.join(data_path,'cars_vald')\n",
    "test_path = os.path.join(data_path,'cars_test')\n",
    "train_label_path = os.path.join(label_path, 'train_perfect_preds.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data analysis 2 things will be done: \n",
    "1. Check for the number of data used in the whole pipeline process. \n",
    "2. Doing T-SNE to the images, the goal is to get sense of the data distribution. (If time is enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_annot = sio.loadmat(os.path.join(label_path, 'cars_annos.mat'))\n",
    "class_names = cars_annot['class_names'][0]\n",
    "df_annot = pd.DataFrame(cars_annot['annotations'][0])\n",
    "df_annot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unused columns, then reformat the remaining data into necessary format for data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annot = df_annot[['relative_im_path','class', 'test']]\n",
    "df_annot = df_annot.applymap(lambda x: x[0])\n",
    "df_annot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annot.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_annot.copy()\n",
    "temp_df['class'] = df_annot['class'].map(lambda x: x[0])\n",
    "temp_df = temp_df.groupby(['class']).count()['test']\n",
    "thresh = 1 / temp_df.size\n",
    "thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df.apply(lambda x: x*100 / temp_df.sum()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print any class with data distribution smaller than 67% of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.67* thresh)\n",
    "temp_df[temp_df < 0.67 * thresh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is none, it safe to assume that the class have relatively even distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train to Test Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_annot.copy()\n",
    "temp_df['class'] = df_annot['class'].map(lambda x: x[0])\n",
    "temp_df['test'] = df_annot['test'].map(lambda x: x[0])\n",
    "temp_df = temp_df.groupby(['class','test']).size()\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_test = temp_df.values[::2]/temp_df.values[1::2]\n",
    "train_to_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE of the images [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an optional step of the process. The goal is to do EDA of the image data using T-SNE, code taken from [here](https://github.com/prabodhhere/tsne-grid). This steps is done to give more sense on what is the data and how it should be handled. The grid image is created by taking 4 images from each classes of car model and make training data, then apply the t-sne algorithm to cluster them and find their best two principal components. Below are the image grid result.\n",
    "\n",
    "To get the image please run two python file __tsne_data_prep.py__ (`python tsne_data_prep.py`) and __tsne_grid.py__ (`python tsne_grid.py --size 28 --dir data/tsne_data/`). Make sure you already have the training data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tsne_grid/tsne_grid.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the data into Train, Validation, and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split part of the *test* data into test and validation set.  \n",
    "Reason for splitting the test rathen the train dataset, is caused by the almost 50%-50% number of train and test size. It seems like a waste to only use so many data for testing.  \n",
    "__Only run it once__, iff you haven't run it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_annot[df_annot['test']==0][['relative_im_path', 'class']]\n",
    "df_test = df_annot[df_annot['test']==1][['relative_im_path', 'class']]\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked, same as the mentioned number from the dataset page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Code to create the validation dataset. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temp_df = df_test.copy()\n",
    "temp_df['class'] = df_test['class'].map(lambda x: x[0])\n",
    "temp_df = temp_df.groupby(['class']).count()['relative_im_path']\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "vald_ratio = 0.3\n",
    "\n",
    "for idx, name in enumerate(class_names):\n",
    "    class_name = '{:03}_{}'.format(idx,name[0].replace('/','_'))\n",
    "    # Train data\n",
    "    train = df_train[df_train['class']==(idx+1)]\n",
    "    create_folder(os.path.join(train_path, class_name))\n",
    "    for item in list(train.items())[0][1]:\n",
    "        copyfile(os.path.join(data_path, item), os.path.join(train_path, item.replace('car_ims',class_name)))\n",
    "    # Test data\n",
    "    temp_df = df_test[df_test['class']==(idx+1)]\n",
    "    mask = np.random.rand(temp_df.shape[0]) < (1-vald_ratio)\n",
    "    test = temp_df[mask]\n",
    "    create_folder(os.path.join(test_path, class_name))\n",
    "    for item in list(test.items())[0][1]:\n",
    "        copyfile(os.path.join(data_path, item), os.path.join(test_path, item.replace('car_ims',class_name)))\n",
    "    # Validation data\n",
    "    vald = temp_df[~mask] \n",
    "    create_folder(os.path.join(vald_path, class_name))\n",
    "    for item in list(vald.items())[0][1]:\n",
    "        copyfile(os.path.join(data_path, item), os.path.join(vald_path, item.replace('car_ims',class_name)))\n",
    "    print(idx, name)\n",
    "    # Save the mapping information just in case it's needed\n",
    "    train.to_csv(os.path.join(data_path, 'labels/train_{}.csv'.format(class_name)),index=False) \n",
    "    vald.to_csv(os.path.join(data_path, 'labels/vald{}.csv'.format(class_name)),index=False)\n",
    "    test.to_csv(os.path.join(data_path, 'labels/test_{}.csv'.format(class_name)),index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_callbacks(weight_folder_path):\n",
    "    create_folder(weight_folder_path)\n",
    "    filepath_acc = os.path.join(weight_folder_path, \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\")\n",
    "    filepath_loss = os.path.join(weight_folder_path, \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "    checkpoint_acc = ModelCheckpoint(filepath_acc, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    checkpoint_loss = ModelCheckpoint(filepath_loss, monitor='val_loss', verbose=1, save_best_only=True, mode='max')\n",
    "    return [checkpoint_acc, checkpoint_loss]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply small value of transformation to the data to help account for variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "vald_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(160, 160),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "validation_generator = vald_datagen.flow_from_directory(\n",
    "        vald_path,\n",
    "        target_size=(160, 160),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(160, 160),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model baseline InceptionV3 will be used to get a minimum model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_model= InceptionV3(include_top=False, weights='imagenet', input_tensor=None, input_shape=(160,160,3), pooling='max')\n",
    "output_l = Dense(196, activation='softmax', name='fc6')(inceptionv3_model.layers[-1].output)\n",
    "inceptionv3_model = Model(inceptionv3_model.input, output_l)\n",
    "inceptionv3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = snapshot.get_callbacks(model_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionv3_model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spe=200\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "history_callback = inceptionv3_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=spe,\n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50, \n",
    "        callbacks=create_basic_callbacks('weight/inceptionv3/'), verbose=1)\n",
    "loss_history = history_callback.history[\"loss\"]\n",
    "np_loss_history = np.array(loss_history)\n",
    "np.savetxt(\"model_inceptionv3_loss_history.txt\", np_loss_history, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = inceptionv3_model.evaluate_generator(test_generator, 100)\n",
    "print(inceptionv3_model.metrics_names)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Model Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complex model *Squeeze-and-Excitation Networks* based on resnet50 will be used. As this model focus on the relationship between the channel it should perform better at task such as detailed classification using transfer learning model. Link to [paper](https://arxiv.org/pdf/1709.01507.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Complex Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the model with learning rate reducer and scheduler.  \n",
    "_Either do model training or load the best weight_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from hollance_model import SEResNet50\n",
    "# importlib.reload(SEResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seresnet_model = SEResNet50(weights=None, input_shape=(160, 160, 3), classes=1000)\n",
    "seresnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the train the model, we will a pretrained model on image-net as the initial weight. Since the only available pre-trained weight is in the form of caffe weight, a parser to keras is needed. The parser is found from [here](https://gist.github.com/hollance/8d30bf5c1622036d16c4f27bd0ec88bf) and slightly modifidied to do the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the network pre-trained weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seresnet_model = SEResNet50(weights=None, input_shape=(160, 160, 3), classes=1000)\n",
    "# seresnet_model.summary()\n",
    "\n",
    "params = np.load(\"SENet2_params.npy\", allow_pickle=True)\n",
    "for key in params[()].keys():\n",
    "    layer_name = key.replace(\"/\", \"_\")   \n",
    "    print(key, \"-->\", layer_name)\n",
    "    layer = seresnet_model.get_layer(layer_name)\n",
    "    layer.set_weights(params[()][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pop the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seresnet_model.layers.pop()\n",
    "output_l = Dense(196, activation='softmax', name='fc6')(seresnet_model.layers[-1].output)\n",
    "seresnet_model = Model(seresnet_model.input, output_l)\n",
    "seresnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "seresnet_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "callbacks_list = create_basic_callbacks('weight/seresnet50_model/') + [lr_reducer,lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seresnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spe=100\n",
    "history_callback = seresnet_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=spe,\n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=10, \n",
    "        callbacks=callbacks_list, verbose=1)\n",
    "loss_history = history_callback.history[\"loss\"]\n",
    "np_loss_history = np.array(loss_history)\n",
    "np.savetxt(\"seresnet_model_loss_history.txt\", np_loss_history, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best trained model with file name 'weights-improvement-93-0.77.hdf5' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seresnet_model.layers.pop()\n",
    "output_l = Dense(196, activation='softmax', name='fc6')(seresnet_model.layers[-1].output)\n",
    "seresnet_model = Model(seresnet_model.input, output_l)\n",
    "seresnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seresnet_model.load_weights('weight/seresnet50_model/weights-improvement-93-0.77.hdf5')\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "seresnet_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = seresnet_model.evaluate_generator(test_generator, 100)\n",
    "print(seresnet_model.metrics_names)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Complex Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SeResNet50 with implementing the [*Stochastic Gradient Descent with Restarts*](https://arxiv.org/abs/1608.03983) ([link to code](https://gist.github.com/jeremyjordan/5a222e04bb78c242f5763ad40626c452)) and [*Snapshot Ensemble* (which require SGDR)](https://arxiv.org/abs/1704.00109) ([link to code](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/callbacks/snapshot.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_contrib.callbacks.snapshot import SnapshotCallbackBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10 # number of snapshots\n",
    "nb_epoch = T = 200 # number of epochs\n",
    "alpha_zero = 0.1 # initial learning rate\n",
    "model_prefix = 'Model_'\n",
    "snapshot = SnapshotCallbackBuilder(T, M, alpha_zero) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = create_basic_callbacks('weight/seresnet50_snapshot/') + snapshot.get_callbacks(model_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SeResnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "history_callback = seresnet_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=int(8000/nb_epoch),\n",
    "        epochs=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50, \n",
    "        callbacks=snapshot.get_callbacks(model_prefix='Model_seresnet'), verbose=1)\n",
    "loss_history = history_callback.history[\"loss\"]\n",
    "np_loss_history = np.array(loss_history)\n",
    "np.savetxt(\"model_seresnet50_loss_history.txt\", np_loss_history, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check result on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = seresnet_model.evaluate_generator(test_generator, 100)\n",
    "print(seresnet_model.metrics_names)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result per time of test: [3.102921153306961, 0.519375]  \n",
    "Meaning bigger loss and lower accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL 36",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
